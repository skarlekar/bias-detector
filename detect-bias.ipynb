{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ['sexuality','neighborhood', 'disability', 'marital status', 'age', 'race', 'ethnicity', 'religion', 'gender']\n",
    "options_str = str.join(',', options)\n",
    "options_str_key = options_str.replace(\" \", \"_\") + \", total_count, highlighted_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"The lender has completed all HUD checks and OFAC checks for all borrowers, and principals, none of which has resulted in need for further investigation. Based on the appraiser's analysis, this development project will add an additional 66 units and associated retail space which will be completed in early 2023.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"The property is in a highly sought-after neighborhood with good schools next to a Hindu temple in a predominantly black neighborhood. The lender has completed all HUD checks and OFAC checks for all borrowers, and principals, none of which has resulted in need for further investigation. Based on the appraiser's analysis, this development project will add an additional 66 units and associated retail space which will be completed in early 2023. The property is surrounded by a white colored fence that runs on the perimeter of the property\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexuality_schema = ResponseSchema(name = \"sexuality\", description = \"Total number of sexual bias found in the given text. Examples of sexual bias is: male-dominated, women-owned.\", type = \"number\")\n",
    "\n",
    "neighborhood_schema = ResponseSchema(name = \"neighborhood\", description = \"Total number of neighborhood bias found in the given text. Examples of neighborhood bias is: highly sought-after neighborhood, good schools, rich community, poor neighborhood.\", type = \"number\")\n",
    "\n",
    "disability_schema = ResponseSchema(name = \"disability\", description = \"Total number of disability bias found in the given text. Examples of disability bias is: mental disorder, blind, deaf, bipolar disorder, ADHD, hearing loss, epilepsy.\", type = \"number\")\n",
    "\n",
    "marital_status_schema = ResponseSchema(name = \"marital_status\", description = \"Total number of marital status bias found in the given text. Examples of marital status bias is: single, married, separated, divorced, widowed\", type = \"number\")\n",
    "\n",
    "age_schema = ResponseSchema(name = \"age\", description = \"Total number of age bias found in the given text. Examples of age bias is: elderly, old person, old people, very young.\", type = \"number\")\n",
    "\n",
    "race_schema = ResponseSchema(name = \"race\", description = \"Total number of race bias found in the given text. Examples of race bias is: white, black, hispanic, asian, native american.\", type = \"number\")\n",
    "\n",
    "ethnicity_schema = ResponseSchema(name = \"ethnicity\", description = \"Total number of ethnicity bias found in the given text. Examples of ethnicity bias is: mexican, african-american, chinese, japanese, indian, french-canadian, indigenous-people, pacific-islander, filipino, cuban.\", type = \"number\")\n",
    "\n",
    "religion_schema = ResponseSchema(name = \"religion\", description = \"Total number of religious bias found in the given text. Examples of religious bias is: hindu temple, church, synagogue, mosque, christian, muslim, hindu, jew.\", type = \"number\")\n",
    "\n",
    "gender_schema = ResponseSchema(name = \"gender\", description = \"Total number of gender bias found in the given text. Examples of gender bias is: male, female, transgender, gender neutral, non-binary, agender, pangender, genderqueer, two-spirit, third gender, and all combination of these.\", type = \"number\")\n",
    "\n",
    "bias_type_count_schema = ResponseSchema(name = 'bias_type_count', description=\"Total count of all type of biases found.\", type = \"number\")\n",
    "\n",
    "highlighted_text_schema = ResponseSchema(name = 'highlighted_text', description=\"Add the highlighted section where the biased phrases are found in the given text. Use Markdown for highlighting the biased phrases that were detected. If no biases are found, leave this empty\")\n",
    "\n",
    "explanation_schema = ResponseSchema(name = 'explanation', description= \"Explain the response and your reasoning here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = {'sexuality': sexuality_schema,'neighborhood': neighborhood_schema, 'disability': disability_schema, 'marital status': marital_status_schema, 'age': age_schema, 'race': race_schema, 'ethnicity': ethnicity_schema, 'religion': religion_schema, 'gender': gender_schema, 'total_count': bias_type_count_schema, 'highlighted_text': highlighted_text_schema, 'explanation_text': explanation_schema}\n",
    "\n",
    "response_schemas = [explanation_schema, bias_type_count_schema, highlighted_text_schema]\n",
    "\n",
    "for option in options:\n",
    "    response_schemas.append(schemas[option])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"explanation\": string  // Explain the response and your reasoning here\n",
      "\t\"bias_type_count\": number  // Total count of all type of biases found.\n",
      "\t\"highlighted_text\": string  // Add the highlighted section where the biased phrases are found in the given text. Use Markdown for highlighting the biased phrases that were detected. If no biases are found, leave this empty\n",
      "\t\"sexuality\": number  // Total number of sexual bias found in the given text. Examples of sexual bias is: male-dominated, women-owned.\n",
      "\t\"neighborhood\": number  // Total number of neighborhood bias found in the given text. Examples of neighborhood bias is: highly sought-after neighborhood, good schools, rich community, poor neighborhood.\n",
      "\t\"disability\": number  // Total number of disability bias found in the given text. Examples of disability bias is: mental disorder, blind, deaf, bipolar disorder, ADHD, hearing loss, epilepsy.\n",
      "\t\"marital_status\": number  // Total number of marital status bias found in the given text. Examples of marital status bias is: single, married, separated, divorced, widowed\n",
      "\t\"age\": number  // Total number of age bias found in the given text. Examples of age bias is: elderly, old person, old people, very young.\n",
      "\t\"race\": number  // Total number of race bias found in the given text. Examples of race bias is: white, black, hispanic, asian, native american.\n",
      "\t\"ethnicity\": number  // Total number of ethnicity bias found in the given text. Examples of ethnicity bias is: mexican, african-american, chinese, japanese, indian, french-canadian, indigenous-people, pacific-islander, filipino, cuban.\n",
      "\t\"religion\": number  // Total number of religious bias found in the given text. Examples of religious bias is: hindu temple, church, synagogue, mosque, christian, muslim, hindu, jew.\n",
      "\t\"gender\": number  // Total number of gender bias found in the given text. Examples of gender bias is: male, female, transgender, gender neutral, non-binary, agender, pangender, genderqueer, two-spirit, third gender, and all combination of these.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = f\"\"\"\n",
    "            For the following text find the total number of biases that indicates a reference to prohibited terms such as: {options_str}.\n",
    "            Example of what is not a bias: \"white fence\", \"black door\", \"brown pen\".\n",
    "            \"\"\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = template_string + \"\"\"\n",
    "text: {text} \n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            For the following text find the total number of biases that indicates a reference to prohibited terms such as: sexuality,neighborhood,disability,marital status,age,race,ethnicity,religion,gender.\n",
      "            Example of what is not a bias: \"white fence\", \"black door\", \"brown pen\".\n",
      "            \n",
      "text: The property is in a highly sought-after neighborhood with good schools next to a Hindu temple in a predominantly black neighborhood. The lender has completed all HUD checks and OFAC checks for all borrowers, and principals, none of which has resulted in need for further investigation. Based on the appraiser's analysis, this development project will add an additional 66 units and associated retail space which will be completed in early 2023. The property is surrounded by a white colored fence that runs on the perimeter of the property \n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"explanation\": string  // Explain the response and your reasoning here\n",
      "\t\"bias_type_count\": number  // Total count of all type of biases found.\n",
      "\t\"highlighted_text\": string  // Add the highlighted section where the biased phrases are found in the given text. Use Markdown for highlighting the biased phrases that were detected. If no biases are found, leave this empty\n",
      "\t\"sexuality\": number  // Total number of sexual bias found in the given text. Examples of sexual bias is: male-dominated, women-owned.\n",
      "\t\"neighborhood\": number  // Total number of neighborhood bias found in the given text. Examples of neighborhood bias is: highly sought-after neighborhood, good schools, rich community, poor neighborhood.\n",
      "\t\"disability\": number  // Total number of disability bias found in the given text. Examples of disability bias is: mental disorder, blind, deaf, bipolar disorder, ADHD, hearing loss, epilepsy.\n",
      "\t\"marital_status\": number  // Total number of marital status bias found in the given text. Examples of marital status bias is: single, married, separated, divorced, widowed\n",
      "\t\"age\": number  // Total number of age bias found in the given text. Examples of age bias is: elderly, old person, old people, very young.\n",
      "\t\"race\": number  // Total number of race bias found in the given text. Examples of race bias is: white, black, hispanic, asian, native american.\n",
      "\t\"ethnicity\": number  // Total number of ethnicity bias found in the given text. Examples of ethnicity bias is: mexican, african-american, chinese, japanese, indian, french-canadian, indigenous-people, pacific-islander, filipino, cuban.\n",
      "\t\"religion\": number  // Total number of religious bias found in the given text. Examples of religious bias is: hindu temple, church, synagogue, mosque, christian, muslim, hindu, jew.\n",
      "\t\"gender\": number  // Total number of gender bias found in the given text. Examples of gender bias is: male, female, transgender, gender neutral, non-binary, agender, pangender, genderqueer, two-spirit, third gender, and all combination of these.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Create a prompt from user input\n",
    "prompt = prompt_template.format_messages (text=user_input, \n",
    "                                format_instructions=format_instructions)\n",
    "print(prompt[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call OpenAI LLM with prompt and receive response\n",
    "response = chat(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"explanation\": \"The text contains several biases related to neighborhood, race, and religion. The phrase 'predominantly black neighborhood' indicates a race bias. The mention of a 'Hindu temple' indicates a religious bias. The phrase 'highly sought-after neighborhood' indicates a neighborhood bias. There are no biases related to sexuality, disability, marital status, age, ethnicity, or gender.\",\n",
      "\t\"bias_type_count\": 3,\n",
      "\t\"highlighted_text\": \"**The property is in a highly sought-after neighborhood with good schools next to a Hindu temple in a predominantly black neighborhood**. The lender has completed all HUD checks and OFAC checks for all borrowers, and principals, none of which has resulted in need for further investigation. Based on the appraiser's analysis, this development project will add an additional 66 units and associated retail space which will be completed in early 2023. The property is surrounded by a white colored fence that runs on the perimeter of the property\",\n",
      "\t\"sexuality\": 0,\n",
      "\t\"neighborhood\": 1,\n",
      "\t\"disability\": 0,\n",
      "\t\"marital_status\": 0,\n",
      "\t\"age\": 0,\n",
      "\t\"race\": 1,\n",
      "\t\"ethnicity\": 0,\n",
      "\t\"religion\": 1,\n",
      "\t\"gender\": 0\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "## Process response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'explanation': \"The text contains several biases related to neighborhood, race, and religion. The phrase 'predominantly black neighborhood' indicates a race bias. The mention of a 'Hindu temple' indicates a religious bias. The phrase 'highly sought-after neighborhood' indicates a neighborhood bias. There are no biases related to sexuality, disability, marital status, age, ethnicity, or gender.\", 'bias_type_count': 3, 'highlighted_text': \"**The property is in a highly sought-after neighborhood with good schools next to a Hindu temple in a predominantly black neighborhood**. The lender has completed all HUD checks and OFAC checks for all borrowers, and principals, none of which has resulted in need for further investigation. Based on the appraiser's analysis, this development project will add an additional 66 units and associated retail space which will be completed in early 2023. The property is surrounded by a white colored fence that runs on the perimeter of the property\", 'sexuality': 0, 'neighborhood': 1, 'disability': 0, 'marital_status': 0, 'age': 0, 'race': 1, 'ethnicity': 0, 'religion': 1, 'gender': 0}\n"
     ]
    }
   ],
   "source": [
    "response_content = output_parser.parse(response.content)\n",
    "print(type(response_content))\n",
    "print(response_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
